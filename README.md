<h1 align="center">Unveiling the Surprising Efficacy of Navigation Understanding in
End-to-End Autonomous Driving</h1>

<p align="center">
<!-- <a href="https://arxiv.org/pdf/2410.08616"><img src="https://img.shields.io/badge/arXiv-Dual_AEB-green"></a> -->
<!-- <a href="https://www.youtube.com/watch?v=KbN-mJnXu08"><img src="https://img.shields.io/badge/YouTube-Video-red?logo=video"></a> -->
<a href="https://github.com/Zhihua-Hua/NavigationDrive/blob/main/LICENSE"><img  src="https://img.shields.io/badge/license-MIT-blue.svg"></a>
</p>

<h4 align="center"><em>
<a href="https://github.com/Zhihua-Hua">Zhihua Hua</a>, 
<a href="https://github.com/wjl2244">Junli Wang</a>, 
<a href="https://github.com/Philipflyg">Pengfei Li</a>,
<a href="#">Qihao Jin</a>, 
<a href="#">Bo Zhang</a>, 
<a href="#">Kehua Sheng</a>, 
<a href="#"> Yilun Chen</a>, 
<a href="#">Zhongxue Gan</a>, 
<a href="#">Wenchao Ding</a>ðŸ“§</em>
</h4>

<h5 align="center">
<b > Fudan University &nbsp; | &nbsp; Didi Chuxing &nbsp; | &nbsp; AIR, Tsinghua University &nbsp; | &nbsp; CASIA </b>
<!-- <a><img align="center" src="assets/logo.png" width="100%"/></a> -->
</h5>

## Abstract
<a align="docs/images/pipeline.png"><img src="assets/pipeline.png"></a>
---
Navigation information serves as a critical component in end-to-end autonomous driving systems, providing essential decision-making references for planner. However, our experimental results reveal that many existing end-to-end autonomous driving systems may not adequately comprehend navigation information, consequently failing to execute appropriate planning based on navigation information. To overcome this limitation, we propose a Sequential Navigation Guidance (SNG) framework, which is designed based on real-world navigation patterns. The SNG incorporates both a navigation path to constrain long-term trajectories and Turn-by-Turn (TBT) information for real-time decision logic. We also introduce an efficient and streamlined model that achieves state-of-the-art (SOTA) performance solely through the accurate modeling of navigation information, without requiring auxiliary loss functions from perception tasks. 
<!-- Automatic Emergency Braking (AEB) systems are a crucial component in ensuring the safety of passengers in autonomous vehicles. Conventional AEB systems primarily rely on closed-set perception modules to recognize traffic conditions and assess collision risks. To enhance the adaptability of AEB systems in open scenarios, we propose Dual-AEB, a system combines an advanced multimodal large language model (MLLM) for comprehensive scene understanding and a conventional rule-based rapid AEB to ensure quick response times. To the best of our knowledge, Dual-AEB is the first method to incorporate MLLMs within AEB systems. Through extensive experimentation, we have validated the effectiveness of our method. -->

<!-- ><strong>[Coming Soon]</strong>: We will release full code after papaer acceptance! Stay tuned for updates on this <a href='#'>page</a> -->

---

## Notes
2025-2-28: **Initialization**.  We will release full code after paper acceptance! 
<!-- Stay tuned for updates on this <a href='#'>page</a> -->
<!-- ><strong>[Coming Soon]</strong>: We will release full code after papaer acceptance! Stay tuned for updates on this <a href='#'>page</a> -->

<!-- ## Citation
If you find this work useful in your research, please consider cite: 

```
``` -->

## Acknowledgments

We thank all the authors who made their codes and datasets public, which tremendously accelerates our project progress. If you find these works helpful, please consider citing them as well.

[Bench2Drive](https://github.com/Thinklab-SJTU/Bench2Drive)
[LLaVA-OneVision](https://github.com/LLaVA-VL/LLaVA-NeXT/)
